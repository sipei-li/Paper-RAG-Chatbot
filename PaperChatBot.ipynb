{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xp2X97Ax0wdL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -q langchain langchain-community langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EHYwhcHE07kB"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "import json\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "\n",
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "idCqYWld1KCL"
      },
      "outputs": [],
      "source": [
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(\n",
        "    model=\"nvidia/nv-embed-v1\",\n",
        "    api_key=userdata.get('NV-EMD-KEY'),\n",
        "    truncate=\"END\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mab7Anwb1LWK"
      },
      "outputs": [],
      "source": [
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(\n",
        "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
        "    api_key=userdata.get('MX-INS-KEY')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVpY8RRAV64"
      },
      "source": [
        "## Loading papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "fNWr_uxQ3abT",
        "outputId": "c6067019-661f-41bd-b768-bc8076aa1834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Documents...\n",
            "...Done!\n",
            "Chunking Documents...\n",
            "...Done!\n",
            "Creating Catalog...\n",
            "...Done!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging </span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging \u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")\n",
        "\n",
        "print(\"Loading Documents...\")\n",
        "docs = []\n",
        "with open('papers.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        docs.append(ArxivLoader(query=line).load())\n",
        "\n",
        "# Cut off references\n",
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "print(\"...Done!\")\n",
        "\n",
        "# Chunk the documents and filter out stubs\n",
        "print(\"Chunking Documents...\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "print(\"...Done!\")\n",
        "\n",
        "print(\"Creating Catalog...\")\n",
        "# Add a catalog chunk\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "print(\"...Done!\")\n",
        "\n",
        "pprint(doc_string, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQR_K7rPDpj-"
      },
      "source": [
        "## Construct Document Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkersD3fB0YU",
        "outputId": "16e4044d-a111-4e64-9d12-6f1d8597e393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructing Vector Stores...\n",
            "...Done!\n",
            "CPU times: user 1.56 s, sys: 172 ms, total: 1.73 s\n",
            "Wall time: 49 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print(\"Constructing Vector Stores...\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]\n",
        "print(\"...Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvDUaRC9EUox",
        "outputId": "99dd59d9-f278-4327-bb77-bf99148edb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging Vectore Stores...\n",
            "...Done\n",
            "Constructed aggregate docstore with 629 chunks.\n"
          ]
        }
      ],
      "source": [
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    \"\"\"Make an empty FAISS vecstore\"\"\"\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False,\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    \"\"\"Initialize an empty FAISS Index and merge others into it.\"\"\"\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vecstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "print(\"Merging Vectore Stores...\")\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "print(\"...Done\")\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP35GrN7GTgb"
      },
      "source": [
        "## RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cOyduvnMId_P"
      },
      "outputs": [],
      "source": [
        "# utilities\n",
        "\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        if preface: print(preface, end=\"\")\n",
        "        pprint(x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string.\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "# Reorder longer documents to center of output text\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
        "\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts {input,output} dict and saves to vstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lBA5f401GRSi"
      },
      "outputs": [],
      "source": [
        "# conversation vecstore\n",
        "convstore = default_FAISS()\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "## -> {input, history, context}\n",
        "stream_chain = chat_prompt | instruct_llm | StrOutputParser()\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'input': (lambda x: x)}\n",
        "    | RunnableAssign({'history' : itemgetter(\"input\") | convstore.as_retriever() | long_reorder | docs2str})\n",
        "    | RunnableAssign({'context' : itemgetter(\"input\") | docstore.as_retriever() | long_reorder | docs2str})\n",
        ")\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    save_memory_and_get_output({'input': message, 'output': buffer}, convstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXiFnUr1KQoO",
        "outputId": "82c4a543-69d8-4e37-9f68-4811f6fd778f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sDREAMER is a model proposed in a study which uses a self-distilled Mixture-of-Modality-Experts (MoME) Transformer for automatic sleep staging. It's a multi-modal learning framework that handles both single-channel and multi-channel inputs. The sDREAMER model emphasizes cross-modality interaction and per-channel performance, which results in high-quality staging results on the sleep staging task.\n",
            "\n",
            "The architecture of the sDREAMER model operates at two levels: epoch and sequence. At the epoch level, it captures epoch-level contexts using an epoch-level MoME module. At the sequence level, a sequence-level MoME transformer captures sequence-level contexts. Even though both levels share the same network architecture, the information flow differs between them during training and inference stages.\n",
            "\n",
            "The sDREAMER model outperforms not only machine-learning-based methods but also deep-learning-based methods on both epoch and sequence levels. The study also tested sDREAMER's ability to generate accurate sleep stage predictions using a single EEG or EMG signal input during the inference stage. The results show that sDREAMER is capable of learning superior mono-modal representations by utilizing multi-modality as a form of supervision during training.\n",
            "\n",
            "Lastly, it's worth mentioning that research reported in the publication was supported by the National Institutes of Health under Award Number U19NS128613. However, the limitation of the model is that its performance is currently compared to one expert. The model may show greater potential if the dataset is scored by more experts.\n",
            "\n",
            "Source: [Quote from sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging]"
          ]
        }
      ],
      "source": [
        "test_question = \"Tell me about sDREAMER\"\n",
        "\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "zw4StSE8KiQ5",
        "outputId": "849fafa6-a9e0-46f1-e70c-025ae202c591"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7203dd3b0e41862e47.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7203dd3b0e41862e47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7203dd3b0e41862e47.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EehPcAgALAC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
